{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b99fe87",
   "metadata": {},
   "source": [
    "## From Fully-Connected Layers to Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5c8c6",
   "metadata": {},
   "source": [
    "in other nn models, we anticipate patterns we seek could involve interactions among features but we do not assume any structure concerning how these features interact. \n",
    "\n",
    "if we do not know how to create crafty architectures to solve such issues, an mlp may just be the best thing one could do. but for high dimensional data, such mlps can grow unweildy. \n",
    "\n",
    "for example, a neural network model that classfies dogs and cats pictures that are of size 1 mega pixel i.e., 1000000 dimensions, and a fully connected neural network of size 1000, would mean 1 billion parameters. even with a lot of patience and very well performing GPUs, it's infeasible. \n",
    "\n",
    "one might argue that we may not need a 1mp image. even if we reduce the quality down to 100000, it's still infeasible with billions of parameters. and fitting lots of parameters require collecting an enormous dataset. yet, humans are easily able to distinguish between them. this is because the images exhibit rich structure that can be exploited by humans and machine learning models alike. CNNs are one creative way to exploit the structure in such images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787860e3",
   "metadata": {},
   "source": [
    "#### Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0403255",
   "metadata": {},
   "source": [
    "spatial invariance: the model's ability to identify targets regardless of it's position in the image. for example the model should be able to identify pigs in the sky and planes on the ground. \n",
    "\n",
    "translation invariance: in the early layers of our network, the model should respond similarly to a patch regardless of it's position in the image. \n",
    "\n",
    "locality principle: in the early layers of our network, the network should focus on local regions regardless of the contents of the image in other distant regions. eventually, these local representations can be aggregated to make predictions at the whole image level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb752d4",
   "metadata": {},
   "source": [
    "#### Constraining the MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf371feb",
   "metadata": {},
   "source": [
    "let X be a two dimensional image which is the input to the network and their hidden representations as H. they both have the same shape which is a two dimensional tensor. we assume that both the image and hidden representations possess spatial structure. \n",
    "\n",
    "we switch from weight matrices to tensors of fourth order. \n",
    "\n",
    "![hidden cell equation](./images/6/6.1.png \"Hidden Layer Equation\")\n",
    "\n",
    "the transformation from W to V is purely cosmetic, to have a one to one correspondence with the weight tensor and the image. \n",
    "\n",
    "k = i + a,\n",
    "\n",
    "l = j + b\n",
    "\n",
    "the indices a and b run over both postitive and negative offsets, covering the entire image. \n",
    "\n",
    "ie., for any given location i, j in the hidden representation H, we compute the value by summing over the pixels in X, centered at i,j weighted by V(i, j, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79651c03",
   "metadata": {},
   "source": [
    "#### Translation Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b150f",
   "metadata": {},
   "source": [
    "translational invariance means a shift in the input should lead to the same shift in the hidden representation H. this is possible when V and U do not depend on i and j. so we simplify the definition of H as follows. \n",
    "\n",
    "![hidden cell equation](./images/6/6.2.png \"Hidden Layer Equation\")\n",
    "\n",
    "this is called a _convolution_\n",
    "\n",
    "we are essentially weighting the pixels at (i+a, j+b) in the vicinity of location (i, j) with coefficients V(a,b) to obtain the value H(i,j).\n",
    "\n",
    "V(a,b) requires a lot less parameters thatn V(i,j,a,b) because we no longer depend on the location within the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e65928",
   "metadata": {},
   "source": [
    "#### Locality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf3dd03",
   "metadata": {},
   "source": [
    "locality means we should not look very far from location (i,j) in order to gain relevant information to assess what's going on at H(i,j). this means we set a limit on a,b exceeding which we set V(a,b) to 0. \n",
    "\n",
    "the updated equation of this becomes\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}$$\n",
    "\n",
    "this equation in a nutshell, is the convolution layer. \n",
    "\n",
    "convolutional neural networks are a family of neural networks that contain these convolution layers. \n",
    "\n",
    "V is referred to as convolution kernel/filter/layer's weights that are learned. \n",
    "\n",
    "with the help of this kernel, the number of paramters go down from millions to a few hundred without altering the dimensionality of the inputs or hidden representations and the features are now translation invariant and the layer can only incorporate local information when being the layer is activated. \n",
    "\n",
    "if the biases do not agree with reality, models will struggle to fit our training data for example the images might not actually be translation invariant, so it struggles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb51d6",
   "metadata": {},
   "source": [
    "#### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafc9c3",
   "metadata": {},
   "source": [
    "in mathematics, the convolution between two functions f and g is defined as $$(f∗g)(x)=∫f(z)g(x−z)dz$$\n",
    "\n",
    "this means we measure the overlap between f and g when one function is _flipped_ and shifted by x. when we have discrete objects, the integral is replaced by sum ( like in the case of convolution kernel)\n",
    "\n",
    "applying the above convolution formula in case of convolution kernel, we get $$(f∗g)(i,j)=∑a∑bf(a,b)g(i−a,j−b)$$\n",
    "\n",
    "the difference between the previous notation and the current notation is i+a, j+b being replaced by i-a and j-b. \n",
    "\n",
    "this difference is purely cosmetic and the original definition describes what is called _cross correlation_. more on this later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eff606",
   "metadata": {},
   "source": [
    "#### Applying above concepts theoretically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867e136",
   "metadata": {},
   "source": [
    "consider the _where's waldo_ problem. we need to learn a model in which the convolution layer picks windows of a given size and weights according to V, and it finds the window where the _waldoness_ is highest. \n",
    "\n",
    "we find the peak in the hidden layer representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015134a",
   "metadata": {},
   "source": [
    "#### Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea824a",
   "metadata": {},
   "source": [
    "the problem with the above approach is, images are not two dimensional objects but are third order tensors where the third order depicts one of three channels; red, green and blue. \n",
    "\n",
    "this means the image representation goes from X(i,j) to X(i,j,k) and similarly the convolution kernel goes from V(a,b) to V(a,b,c)\n",
    "\n",
    "now the equation for the hidden representation is adapted to work with the third order tensors as\n",
    "\n",
    "$$[H]i,j,d=∑a=−ΔΔ∑b=−ΔΔ∑c[V]a,b,c,d[X]i+a,j+b,c$$\n",
    "\n",
    "here d indexes the output channels in the hidden represeentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8ec2a",
   "metadata": {},
   "source": [
    "## Convolutions to Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a324bc0d",
   "metadata": {},
   "source": [
    "#### The Cross-Correlation Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc29996",
   "metadata": {},
   "source": [
    "the _convolution_ seen before is a misnomer as the operations can more accurately be expressed as cross correlations where input tensor and a kernel tensor are combined to produce an output tensor. \n",
    "\n",
    "if we ignore channels, this is how it works \n",
    "\n",
    "![Convolution Layer](./images/6/6.4.png \"Convolution Layer\")\n",
    "\n",
    "the convolution slides over the input tensor. the kernel is smaller than the input window. the size of output can be deduced as $$(nh−kh+1)×(nw−kw+1)$$ where nh and nw are heights and widths of inputs and kh and kw are heights and widths of kernel. \n",
    "\n",
    "we can pad the input image with zeros around it, to get hidden representations of the same size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29c6ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j].assign(tf.reduce_sum(X[i:i+h, j:j+w] * K))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95c78f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
       "array([[19., 25.],\n",
       "       [37., 43.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.constant([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = tf.constant([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa8332",
   "metadata": {},
   "source": [
    "#### Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234d457",
   "metadata": {},
   "source": [
    "a convolution layer cross correlates the input and kernel and adds a scalar bias to produce an output ( the hidden representation). the two parameters are the kernel and scalar bias. when training models on convolution layers, we typically initialize them randomly, like we do in fully connected layers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2736562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def build(self, kernel_size):\n",
    "        initializer = tf.random_normal_initializer()\n",
    "        # weight w\n",
    "        self.weight = self.add_weight(name = 'w', shape = kernel_size, initializer = initializer)\n",
    "        # bias u\n",
    "        self.bias = self.add_weight(name = 'b', shape = (1, ), initializer = initializer)\n",
    "    def call(self, inputs):\n",
    "        # WX + U\n",
    "        return corr2d(inputs, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66852eec",
   "metadata": {},
   "source": [
    "#### Object Edge Detection in Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e5fea",
   "metadata": {},
   "source": [
    "creating an \"image\" of size 6x8, middle four columns are black and the rest white. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b54ad25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(6, 8) dtype=float32, numpy=\n",
       "array([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.Variable(tf.ones((6, 8)))\n",
    "X[:, 2:6].assign(tf.zeros(X[:, 2:6].shape))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f902b1f",
   "metadata": {},
   "source": [
    "creating a 1x2 kernel. when we perform the cross correlation operation, if horizontally adjacent elements are same, input is 0, otherwise non zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9e2498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tf.constant([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35709718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(6, 7) dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf4fbe",
   "metadata": {},
   "source": [
    "the kernel only detects vertical edges ie., it won't work for horizontal edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47884128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(8, 5) dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(tf.transpose(X), K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d59250",
   "metadata": {},
   "source": [
    "#### Learning a Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352b7a0",
   "metadata": {},
   "source": [
    "initializing a kernel to [1,-1] to detect edges in a small image makes sense. but for larger inputs and multiple convolution layers, it will be impossible to specify precisely what each filter should be. \n",
    "\n",
    " we first initialize the kernel as a random tensor, then use squared error to compare Y with output of convolution layer and calculate the gradient to update the kernel. \n",
    " \n",
    " using the built in keras conv layer for simplicity and ignoring the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04f740e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1, Loss: 0.078\n",
      "Batch: 2, Loss: 0.050\n",
      "Batch: 3, Loss: 0.032\n",
      "Batch: 4, Loss: 0.020\n",
      "Batch: 5, Loss: 0.013\n",
      "Batch: 6, Loss: 0.008\n",
      "Batch: 7, Loss: 0.005\n",
      "Batch: 8, Loss: 0.003\n",
      "Batch: 9, Loss: 0.002\n",
      "Batch: 10, Loss: 0.001\n"
     ]
    }
   ],
   "source": [
    "conv2d = tf.keras.layers.Conv2D(1, (1, 2), use_bias = False)\n",
    "\n",
    "# the two dimensional conv layer uses 4 dimensional inputs and outputs. \n",
    "# they are in the format of (example, height, width, channel) and the batch size and number of channels are 1. \n",
    "X = tf.reshape(X, (1, 6, 8, 1))\n",
    "Y = tf.reshape(Y, (1, 6, 7, 1))\n",
    "# learning rate\n",
    "lr = 3e-2\n",
    "\n",
    "Y_hat = conv2d(X)\n",
    "\n",
    "for i in range(10):\n",
    "    with tf.GradientTape(watch_accessed_variables = False) as g:\n",
    "        g.watch(conv2d.weights[0])\n",
    "        Y_hat = conv2d(X)\n",
    "        l = (abs(Y_hat - Y)**2)\n",
    "        # updating the kernel\n",
    "        update = tf.multiply(lr, g.gradient(l, conv2d.weights[0]))\n",
    "        weights = conv2d.get_weights()\n",
    "        weights[0] = conv2d.weights[0] - update\n",
    "        conv2d.set_weights(weights)\n",
    "        print(f\"Batch: {i+1}, Loss: {tf.reduce_sum(l):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea129870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.9961692, -1.0038975]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(conv2d.get_weights()[0], (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3314017",
   "metadata": {},
   "source": [
    "#### Cross-Correlation and Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feebb565",
   "metadata": {},
   "source": [
    "in order to perform strict convolution instead of cross correalation, we need to flip the two dimensional kernel both horizontally and vertically and then perform cross correlation operation. \n",
    "\n",
    "since these kernels are learned from data, it doesn't matter whether the convolution layers perform cross correlation or convolution. \n",
    "\n",
    "if K is the kernel learned from cross-correlation and K' is learned from strict convolution, K' will be the same as K when flipeed both horizontally and vertically. \n",
    "\n",
    "in standard deep learning terminology, cross-correlation is often referred to as the convolution even though they are slightly different. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b07ccb",
   "metadata": {},
   "source": [
    "#### Feature Map and Receptive Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04055857",
   "metadata": {},
   "source": [
    "the convolution layer's output is often referred to as feature map as it can be regarded as the learned features/representations to the subsequent layer.\n",
    "\n",
    "for any element , receptive field refers to all elements from all previous layers that may effect the calculation of x during forward propogation. receptive field may be larger than the actual input as it is the set of all previous values before an element. \n",
    "\n",
    "![Convolution Layer](./images/6/6.4.png \"Convolution Layer\")\n",
    "\n",
    "for example, in the above image, given the 2x2 kernel, receptive field of 19, is the four elements in the shaded portion of input. now assume, the output of this layer, Y,  is the input for another layer which outputs a single value z. the receptive field of z on Y includes all four elements of Y, but the receptive field of z on input includes all the nine input elements. \n",
    "\n",
    "thus if we need more elements in the receptive field of an element in a feature map, we create deeper networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d17685",
   "metadata": {},
   "source": [
    "## Padding and Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad480d2d",
   "metadata": {},
   "source": [
    "as discussed previously, if nh and nw are heights and widths of the input and kh and kw are the height and width of the kernel, the output shape of convolution kernel by default would be (nh-kh+1)x(nw-kw+1).\n",
    "this means after multiple convolutions, we end up with image shapes that are much smaller than the input shape. \n",
    "\n",
    "this can be handled by incorporating padding. in some other cases, we might want to reduce the dimensionality of the input shape drastically. in such cases we make use of striding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ab0ab",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16da318",
   "metadata": {},
   "source": [
    "we basically add filler pixels around the input shape and typically set these filler pixels to 0.\n",
    "\n",
    "![Padding](./images/6/6.5.svg \"Padding\")\n",
    "\n",
    "in general, if we add ph rows and pw columns of padding, the output shape will be\n",
    "\n",
    "(nh - kh + ph + 1)x(n - kw + pw + 1)\n",
    "\n",
    "in many cases wew set up ph = kh - 1 and pw = kw - 1 to get input and output shapes of same heights and widths. \n",
    "\n",
    "if kh or kw is odd, we pad with ph/2 or pw/2. \n",
    "\n",
    "cnns commonly use convolution kernels with odd height and width values such as 1,3,5,7. choosing odd kernel sizes has the benefit that we can preserve the spatial dimensionality while padding with the same number of rows on top and bottom and the same number of columns on left and right. \n",
    "\n",
    "another benefit of using odd shaped convolution kernels and padding to get outputs that are the same shape as inputs is, the output Y[i,j] is a direct result of cross correlation of the input centered at X[i,j]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "767720ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 8])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def comp_conv2d(conv2d, X):\n",
    "    \"\"\"this function performs reshaping of input and outputs produced by applying the conv2d layer\"\"\"\n",
    "    # (1,) and (1,) denote batch size and number of channels respectively.\n",
    "    X = tf.reshape(X, (1,) + X.shape + (1,))\n",
    "    Y = conv2d(X)\n",
    "    return tf.reshape(Y, Y.shape[1:3])\n",
    "\n",
    "# padding = 'same' pads the input in such a way that the output and input are the same shape. \n",
    "# in this case it adds 2 rows and 2 columns. \n",
    "conv2d = tf.keras.layers.Conv2D(1, kernel_size = 3, padding = 'same')\n",
    "X = tf.random.uniform(shape = (8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecddd4d",
   "metadata": {},
   "source": [
    "the output and input are the same shape as a result of padding = 'same' no matter the shape of kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b872bcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = tf.keras.layers.Conv2D(1, kernel_size = (5, 3), padding = 'same')\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b2601",
   "metadata": {},
   "source": [
    "#### Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564713cb",
   "metadata": {},
   "source": [
    "in normal cross correlation, we moved the kernel over the input one element at a time. however because of needs to decrease computation time or to downsample the input, this can be done by skipping more than one element at at time. \n",
    "\n",
    "we refer to the number of rows and columns traversed per slide as a _stride_. \n",
    "\n",
    "![\"3 2 striding\"](./images/6/6.6.svg)\n",
    "\n",
    "in the above image, cross correlation is applied with strides of 3 and 2 for height and width respectively.\n",
    "kernel slides 2 columns horizontally till it's unable to fit one more window (unless more padding is applied)\n",
    "and it slides 3 columns vertically to produce the output. \n",
    "\n",
    "when stride for height is sh and stride for width is sw, the output shape is\n",
    "\n",
    "$$⌊(nh−kh+ph+sh)/sh⌋×⌊(nw−kw+pw+sw)/sw⌋$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be6edafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = tf.keras.layers.Conv2D(1, kernel_size = 3, padding = 'same', strides = 2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c82ab",
   "metadata": {},
   "source": [
    "the input got downsampled because of the strides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a456307e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding = 'valid' means no artifical zeros are added to the data. it drops actual data if needed. \n",
    "conv2d = tf.keras.layers.Conv2D(1, kernel_size = (3, 5), padding = 'valid', strides = (3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1794227",
   "metadata": {},
   "source": [
    "## Multiple Inputs and Output Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98bde2",
   "metadata": {},
   "source": [
    "with one channel, we were able to think of inputs, outputs and kernels as two dimensional matrices. but with the addition of channels, they become three dimensional tensors. \n",
    "\n",
    "for example, input becomes 3 x h x w. this 3 is referred to as channel dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5608a2",
   "metadata": {},
   "source": [
    "#### Multiple Input Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398312f",
   "metadata": {},
   "source": [
    "when there are multiple channels, the convolution kernel should have the same number of channels as the input so that cross correlation can be performed. \n",
    "\n",
    "if ci is the number of channels, then there needs to be a kernel for each of the channel, so the shape of the kernels will be ci x kh x kw. \n",
    "\n",
    "since there will be ci channels with each channel having it's own kernel, two dimentional convolution operation can be performed on the two dimensional input with the kernel. this is the result of two dimensional cross correlation between a multi channel input and a multi channel convolution kernel. \n",
    "\n",
    "![multi channel cross convolution](./images/6/6.7.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fe543df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    return tf.reduce_sum([corr2d(x, k) for x, k in zip(X, K)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2dbe152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 56.,  72.],\n",
       "       [104., 120.]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.constant([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = tf.constant([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c52c2",
   "metadata": {},
   "source": [
    "#### Multiple Output Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36606a10",
   "metadata": {},
   "source": [
    "so far, the output has always been single channeled. but it's essential that all outputs have multiple channels because they are passed as inputs to the next layers. \n",
    "\n",
    "in most popular neural network architectures, we purposefully increase the channel dimension as we go deeper into the architecture, which typically downsamples the data for greater channel depth. \n",
    "\n",
    "intiutively, these multiple channels can be thought of as different sets of features learnt, but in reality, these channels are not learned independent but are rather optimized to be jointly useful. \n",
    "\n",
    "assume ci and co are input and output channels, kh and kw are height and width of kernels. \n",
    "\n",
    "we create a kernel ternsor of shape ci x kh x kw for each layer of input and concatenate them on the output channel dimension to get co x ci x kh x kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1333328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    \"\"\"iterate through the 0th dimension of k and each time perform cross correaltion operations with X. \n",
    "    the results are then stacked together\"\"\"\n",
    "    return tf.stack([corr2d_multi_in(X, k) for k in K], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ec24be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 2, 2, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = tf.stack((K, K+1, K+2), 0)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9a56e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
       "array([[[ 56.,  72.],\n",
       "        [104., 120.]],\n",
       "\n",
       "       [[ 76., 100.],\n",
       "        [148., 172.]],\n",
       "\n",
       "       [[ 96., 128.],\n",
       "        [192., 224.]]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271a6a9",
   "metadata": {},
   "source": [
    "#### 1x1 Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f0400",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
