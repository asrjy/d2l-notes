{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 6 - Convolutional Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSQJfvfrpsUG07IbBLvAB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0abea00cd4a24136a4e836ac0da20153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b0de0d0bdfd483c92bfb470b38e31fb",
              "IPY_MODEL_222b200164f2452aa3e05ade07452bc3",
              "IPY_MODEL_22d270a850e54373a93b3481e7eef240"
            ],
            "layout": "IPY_MODEL_4dcf6bf037414bc984560b0ec25f0a92"
          }
        },
        "1b0de0d0bdfd483c92bfb470b38e31fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4705184402d842b5b6877735459e7bec",
            "placeholder": "​",
            "style": "IPY_MODEL_c2c07779d9aa41f8bf5d9e10ace7e8ff",
            "value": ""
          }
        },
        "222b200164f2452aa3e05ade07452bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24634db0f5464613a80fca5e4cbba3c6",
            "max": 26421880,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6da76f94ee00470a87c1a2a3aa44a942",
            "value": 26421880
          }
        },
        "22d270a850e54373a93b3481e7eef240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a6ce7131a4f4274b697bd818e034983",
            "placeholder": "​",
            "style": "IPY_MODEL_af46a82fb3f04207a8c5196c231068b5",
            "value": " 26422272/? [00:01&lt;00:00, 23906269.02it/s]"
          }
        },
        "4dcf6bf037414bc984560b0ec25f0a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4705184402d842b5b6877735459e7bec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c07779d9aa41f8bf5d9e10ace7e8ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24634db0f5464613a80fca5e4cbba3c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6da76f94ee00470a87c1a2a3aa44a942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a6ce7131a4f4274b697bd818e034983": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af46a82fb3f04207a8c5196c231068b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "223e59734a1242da9d6c43d04972b4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e390176548d4c67b0d3e59e8d51e6aa",
              "IPY_MODEL_7c4891aedf9a4a49aee2b46802a84d7e",
              "IPY_MODEL_b5e1a423a432423086a1793887a7ec79"
            ],
            "layout": "IPY_MODEL_83a99d6726ea49249d30fdae90e47d76"
          }
        },
        "0e390176548d4c67b0d3e59e8d51e6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee668d4b5a894e8699658bfbd757c230",
            "placeholder": "​",
            "style": "IPY_MODEL_74ecb840b18a4a6fa051798a669b47e8",
            "value": ""
          }
        },
        "7c4891aedf9a4a49aee2b46802a84d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29aa70c2a43640acad4969cca40c6d8b",
            "max": 29515,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f0f9a08f3a343bb9ccda6ca3d4b266e",
            "value": 29515
          }
        },
        "b5e1a423a432423086a1793887a7ec79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_352fa135387544f69d492dadf67e4f65",
            "placeholder": "​",
            "style": "IPY_MODEL_f0efabcc66a749c2b067f366ebb5750f",
            "value": " 29696/? [00:00&lt;00:00, 299874.45it/s]"
          }
        },
        "83a99d6726ea49249d30fdae90e47d76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee668d4b5a894e8699658bfbd757c230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74ecb840b18a4a6fa051798a669b47e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29aa70c2a43640acad4969cca40c6d8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f0f9a08f3a343bb9ccda6ca3d4b266e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "352fa135387544f69d492dadf67e4f65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0efabcc66a749c2b067f366ebb5750f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "160cd30044754d29af5ad63f423a58e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c63a0e2cc9404454ade4708c41ee6179",
              "IPY_MODEL_d9f85803fea94a2e80880e76e2d2bf51",
              "IPY_MODEL_c3547e3e1e2b4f3cac9b30c0ac3381b7"
            ],
            "layout": "IPY_MODEL_ac47bb7b8df84b01ba77b68117c4d9fa"
          }
        },
        "c63a0e2cc9404454ade4708c41ee6179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46a89b9d41f74ab88791977a81db80bf",
            "placeholder": "​",
            "style": "IPY_MODEL_b13904f7d3d94b82b0b642507faab214",
            "value": ""
          }
        },
        "d9f85803fea94a2e80880e76e2d2bf51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_507ca77ce1104125ac14be00de1244d0",
            "max": 4422102,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af5d8696c72e4acea447dad96c9e45bb",
            "value": 4422102
          }
        },
        "c3547e3e1e2b4f3cac9b30c0ac3381b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b85a8815f5cb438baa25bd4cdf754001",
            "placeholder": "​",
            "style": "IPY_MODEL_f4f89ea24db04c1d927fde8e54638d6b",
            "value": " 4422656/? [00:00&lt;00:00, 9608402.57it/s]"
          }
        },
        "ac47bb7b8df84b01ba77b68117c4d9fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46a89b9d41f74ab88791977a81db80bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b13904f7d3d94b82b0b642507faab214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "507ca77ce1104125ac14be00de1244d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af5d8696c72e4acea447dad96c9e45bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b85a8815f5cb438baa25bd4cdf754001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4f89ea24db04c1d927fde8e54638d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2c01048c3f041a6a3e48fd764542ee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d59b8d9073734afaa74df03de1062403",
              "IPY_MODEL_6f85b3b8c00e424c93218f791b47c0d8",
              "IPY_MODEL_c84413b91df14caf8a8773f8eb308725"
            ],
            "layout": "IPY_MODEL_af9a59eb65ef449595ba3aecbcd55a76"
          }
        },
        "d59b8d9073734afaa74df03de1062403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed4778178d884b2484078c576ded399c",
            "placeholder": "​",
            "style": "IPY_MODEL_e57f16665ef2425fbe1a2730de32963f",
            "value": ""
          }
        },
        "6f85b3b8c00e424c93218f791b47c0d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d60ea21eccd4402394c1d1f73783778f",
            "max": 5148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80229366c7074a15a8e509387eb48f8e",
            "value": 5148
          }
        },
        "c84413b91df14caf8a8773f8eb308725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95d20b3036064733acb0b38e6c107fdc",
            "placeholder": "​",
            "style": "IPY_MODEL_3575cb9289744bd2a4fa3af248229711",
            "value": " 6144/? [00:00&lt;00:00, 124155.33it/s]"
          }
        },
        "af9a59eb65ef449595ba3aecbcd55a76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed4778178d884b2484078c576ded399c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e57f16665ef2425fbe1a2730de32963f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d60ea21eccd4402394c1d1f73783778f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80229366c7074a15a8e509387eb48f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95d20b3036064733acb0b38e6c107fdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3575cb9289744bd2a4fa3af248229711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asrjy/d2l-notes/blob/master/Chapter%206%20-%20Convolutional%20Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "Flattening image layers means losing the spatial relationship between the pixels. In order to overcome this issue, we use CNNs instead of regular MLPs\n",
        "\n",
        "Since MLPs don't care about the spatial relationship, we get the same results even though the order of the pixels are modified. This phenomenon is not wanted. \n",
        "\n",
        "Modern CNNs tend to be computationally effieicent, require fewer parameters than fully connected networks and easy to parallelize over multiple GPU cores. CNNs have also shown decent performance on one dimenstionals data such as audio, time series data where recurrent neural networks are conventionally used. \n",
        "\n",
        "## From Fully Connected Layers to Convolutions\n",
        "\n",
        "For low dimensional data where we lack the knowledge to construct specific architectures that identify patterns that seek interactions among features, an MLP may be the best we can do. However, for high dimensional perceptual data, such structure less networks can gro unwieldy. \n",
        "\n",
        "Convolutional Neural Networks are one way to exploit the structure in natural images. \n",
        "\n",
        "### Invariance\n",
        "\n",
        "Spatial Invariance allows CNNs to learn useful representations with fewer parameters. What it basically means is our model should identify a pig in air or a plane in water. Where the target is located is not of importance, it's existence is what we look for. \n",
        "\n",
        "There are two different types of invariances. \n",
        "\n",
        "1 - In the earliest layers, our network should respond similarly to the same patch, regardless of where the patch is located in the image. A patch here means a part of the image on which the NN works on. This principle is called translation invariance / translation equivariance. \n",
        "\n",
        "2 - The earliest layers of the network should focus on local regions, without regard for the contents in distant regions. This is called locality principle. Eventually they are aggregated to make predictions on the image as a whole. \n",
        "\n",
        "3 - As we proceed, deeper layers should capture longer range features of the image similar to higher level vision in nature. \n",
        "\n",
        "### Constraining the MLP\n",
        "\n",
        "Assume the image is represented as X with shape (i, j) and the hidden representation is represented as H with shape also (i, j) where each element of the hidden representation is calculated by summing over the pixels of X centered at i, j weighted by V (i, j, a, b)\n",
        "\n",
        "\\begin{split}\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l}\\\\ &=  [\\mathbf{U}]_{i, j} +\n",
        "\\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b}  [\\mathbf{X}]_{i+a, j+b}.\\end{aligned}\\end{split}\n",
        "\n",
        "The indices a, b run over both positive and negative offsets ocvering the entire image. \n",
        "\n",
        "#### Translation Invariance\n",
        "\n",
        "According to this principle, a shift in X should lead to a shift in the hidden representation. This is only possible if V and U do not depepnd on i, j. As a result we represent V(i, j, a, b) as just V((a, b) and U as a constant. \n",
        "\n",
        "Now the simplified representation is \n",
        "\n",
        "$[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}$.\n",
        "\n",
        "The above concept is called convolution where we are effectively weighing pixels at (i+a j+b) in the vicinity of (i, j) with coefficients V(a, b) to obtain the hidden representation. \n",
        "\n",
        "V(a, b) requires far fewer parameters than V(i, j, a, b) since it no longer depends on i and j. \n",
        "\n",
        "#### Locality\n",
        "\n",
        "According to this principle, we should not look very far from X(i, j) to get relevant information about what is going on at X(i, j). This means, after some value of a and b, V(a, b) need to be 0. \n",
        "\n",
        "Now the hidden representation becomes, \n",
        "\n",
        "$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}$.\n",
        "\n",
        "The value of delta is typically smaller than 10. \n",
        "\n",
        "The above equation is called a convolutional layer. V is referred to as Convolution kernel/filter. \n",
        "\n",
        "Without this layer, for a single megeapixel image, we would require billions of parameters, but with the convolutional layer, we would require a few hundred, without altering the dimensionality of either the inputs or the hidden representations. \n",
        "\n",
        "The cost of this reduction of in parameters is that, we only capture local information while determining the value of each hidden activation. \n",
        "\n",
        "This bias might not always agree with reality, as there could be images that are not translation invariant. \n",
        "\n",
        "### Convolutions\n",
        "\n",
        "In mathematics, a convolution operation between two functions is the measure of overlap between f and g when g is flipped and shifted by x. \n",
        "\n",
        "$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}.$\n",
        "\n",
        "When we are dealing with discrete objects, the integral in the beginning, becomes a sum. \n",
        "\n",
        "$(f * g)(i) = \\sum_a f(a) g(i-a).$\n",
        "\n",
        "For two dimensional tensors, we have corresponding indices a and b for i and j. \n",
        "\n",
        "$(f * g)(i, j) = \\sum_a\\sum_b f(a, b) g(i-a, j-b).$\n",
        "\n",
        "This is similar to the convolution operation we arrived at before excluding the + instead of -. The more proper name for the equation we got before is cross-correlation. \n",
        "\n",
        "### Channels\n",
        "\n",
        "To support multiple channels in both the inputs and the hidden representations, we add two more coordinates to V and one more to X. \n",
        "\n",
        "$[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c},$\n",
        "\n",
        "where d indexes the output channels in the hidden representation. \n",
        "\n",
        "### Exercises \n",
        "\n",
        "1 - Assume that the size of the convolution kernel is $\\Delta = 0$. Show that in this case the convolution kernel implements an MLP independently for each set of channels. This leads to the Network in Network architectures [Lin et al., 2013].\n",
        "\n",
        "Ans - When the size of convolution layer is 0, it means, nearby data of each coordinate is not considered. There's no convolution happening. The shape of V and X are going to be the same. This is what happens in a regular MLP. \n",
        "\n",
        "2.1 - Audio data is often represented as a one-dimensional sequence. When might you want to impose locality and translation invariance for audio?\n",
        "\n",
        "Ans - Inclusion of locality and translation invariance means, audio at one part is not necessarily related to audio at another part. This wouldn't work for speech related problems, but would work for pattern recognition problems where local part doesnt have any relation to parts outside its locality.\n",
        "\n",
        "3 - Why might translation invariance not be a good idea after all? Give an example.\n",
        "\n",
        "Ans - Not a good idea in cases where one patch of the image has a relationship with it's position. An example could be when identifying the face of a human, if the position of eyes and mouth are exchanged, the prediction probably would not be a human. \n",
        "\n",
        "4 - Do you think that convolutional layers might also be applicable for text data? Which problems might you encounter with language?\n",
        "\n",
        "Ans - Convolutional Layers include translational invariance and Locality. Translation Invariance would not work on text data as it is sequential data. Locality would not be of much help since words make sense only when combined with other words. \n",
        "\n",
        "5 - What happens with convolutions when an object is at the boundary of an image.\n",
        "\n",
        "Ans - Values around the border are decided using padding. \n",
        "\n",
        "## Convolutions for Images\n",
        "\n",
        "### The Cross-Correlation Operation\n",
        "\n",
        "If the input is of size n_h \\times n_w and the kernel is of size $k_h \\times k_w$, the output after the cross correlation operation would be of size $(n_h-k_h+1) \\times (n_w-k_w+1).$\n",
        "\n",
        "This is because we need enough space to shift the kernel across the image. With padding, the output size will not be varied as we pad zeros around the boundary so there is enough space to shift the kernel. "
      ],
      "metadata": {
        "id": "0edxHfhNcibH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def corr2d(X, K):\n",
        "  h, w = K.shape\n",
        "  Y = torch.zeros((X.shape[0] - h + 1) , (X.shape[1] - w + 1))\n",
        "  for i in range(Y.shape[0]):\n",
        "    for j in range(Y.shape[1]):\n",
        "      Y[i, j] = (X[i: i+h, j:j+w] * K).sum()\n",
        "  return Y "
      ],
      "metadata": {
        "id": "iclTf9TSci_T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "corr2d(X, K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkDanouF-LHt",
        "outputId": "eac53867-b660-4645-bb62-92b6f5ee3b5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[19., 25.],\n",
              "        [37., 43.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Layers\n",
        "\n",
        "A convolutional layer performs cross correlation  between input and kernel and adds a scalar bias to it to produce am output. The two parameters of a convolutional layer are the kernel and the bias. When training, we typically intialize them randomly. \n",
        "\n",
        "Implementing a convolutional layer based on the corr2d function defined above. "
      ],
      "metadata": {
        "id": "WsuHbdod-dKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2D(nn.Module):\n",
        "  def __init__(self, kernel_size):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.rand(kernel_size))\n",
        "    self.bias = nn.Parameter(torch.zeros(1))\n",
        "  def forward(self, X):\n",
        "    return corr2d(X, self.weight) + self.bias"
      ],
      "metadata": {
        "id": "DqtXgXK4B0kt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Edge Detection in Images\n",
        "\n",
        "Detecting the edge of an object can be performed by finding the location of change of color in piels. "
      ],
      "metadata": {
        "id": "uo0Xd6nNCQb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.ones((6, 8))\n",
        "X[:, 2:6] = 0\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmPsVBgICeZu",
        "outputId": "1d540661-20d9-406a-8b7a-36adabd01fca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we perform cross correlation with a kernel of size 1x2, if the two elements match with the elements in the patch of the image, it outputs 0 ie., at location i, j it calculates x(i, j) - x(i+1, j)"
      ],
      "metadata": {
        "id": "TJuqUi1OCkBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = torch.tensor([[1.0, -1.0]])"
      ],
      "metadata": {
        "id": "J85n9at2DfA6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = corr2d(X, K)\n",
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l64kHQzIDvgd",
        "outputId": "19e4f91d-fea6-4a8b-cf25-bdb0220dfd4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr2d(X.t(), K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAfeaR0QDy__",
        "outputId": "32e64492-19d2-4a70-e563-420105014af3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does not work because, the cross correlation can only detect vertical edge, not horizontal edges. \n",
        "\n",
        "### Learning a Kernel\n",
        "\n",
        "Although the above process is nice, it's hard to define each kernel when we are working with bigger architectures. We want to kernel to automatically learn these processes. \n",
        "\n",
        "First we construct a convolution layer and initialize it randomly. In each iteration we will use the squared error to compare Y with the output of the convolution layer, calculate the gradient to update the kernel. "
      ],
      "metadata": {
        "id": "hbci4EDoECar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional Layer with 1 output channel and kernel fo shape (1, 2). Ignoring the bias for now. \n",
        "conv2d = nn.LazyConv2d(1, kernel_size = (1, 2), bias = False)\n",
        "\n",
        "# (Example, Channel, Height, Width)\n",
        "X = X.reshape((1, 1, 6, 8))\n",
        "Y = Y.reshape((1, 1, 6, 7))\n",
        "lr = 3e-2\n",
        "\n",
        "for i in range(10):\n",
        "  Y_hat = conv2d(X)\n",
        "  l = (Y_hat - Y) ** 2\n",
        "  conv2d.zero_grad()\n",
        "  l.sum().backward()\n",
        "  # Updating the kernel\n",
        "  conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
        "  if (i+1)%2 == 0:\n",
        "    print(f\"Epoch {i+1}, loss {l.sum():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM20r8_YEMbU",
        "outputId": "47283bbf-9d82-41be-96c1-4ddaf7c9c579"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, loss 11.266\n",
            "Epoch 4, loss 2.136\n",
            "Epoch 6, loss 0.459\n",
            "Epoch 8, loss 0.118\n",
            "Epoch 10, loss 0.037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d.weight.data.reshape((1, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRAH1M0gHICj",
        "outputId": "fa5e544f-2f12-4439-a721-6a5055926cd9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0008, -0.9659]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Correlation and Convolution\n",
        "\n",
        "In order to perform just convolution instead of cross correlation, we need to flip the kernel both horizontally and vertically, then perform cross correlation with the input tensor. \n",
        "\n",
        "Since kernels are learnt from data, it doesnt matter whether layers perform cross correlation or convolution. The output remains the same. \n",
        "\n",
        "Meaning if a layer performs cross correlation and it's weights are represented as K, the learned kernel be K', K' will be the same even when K is flipped horizontally and vertically. \n",
        "\n",
        "### Feature Map and Receptive Field\n",
        "\n",
        "The outpt of Convolution Layer is sometimes called Feature Map. Receptive field of any element x of any layer means the elements from the previous layers, that cal affect the calculation of x during the forward propagation. It may be larger than the actual size of the input. "
      ],
      "metadata": {
        "id": "90BFBg_PLyv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding and Stride \n",
        "\n",
        "10 Convolution Layers of kernel size 5x5 on a 240x240 image, reduce the output size to 200x200. Padding and Strided Convolutions offer more control over the size of the output. \n",
        "\n",
        "### Padding\n",
        "\n",
        "One straightforward solution to this issue is to add zeros around the image. If we add ph rows of padding (half on top and the rest on bottom) and pw columns of padding (half on left and the rest on right), the output shape would be \n",
        "\n",
        "(nh - kh + ph + 1) x (nw - kw + pw + 1). \n",
        "\n",
        "In many cases ph = kh - 1 and pw = kw - 1 to give input and output the same height and width. \n",
        "\n",
        "CNNs commonly use conv kernels of odd height and width. This means when we pad, we can divide the number of rows and columns by 2, thus having equal row and column paddings on both sides. \n"
      ],
      "metadata": {
        "id": "wUtCkMtgUA7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def comp_conv2d(conv2d, X):\n",
        "  # conv2d requires images of 4 dimensions. Adding the example and channel dimension\n",
        "  X = X.reshape((1, 1) + X.shape)\n",
        "  Y = conv2d(X)\n",
        "  # stripping the example and channel dimensions\n",
        "  return Y.reshape(Y.shape[2:])\n",
        "\n",
        "# 1 row and column are padded on either side, so total of 2 rows or columns are added\n",
        "conv2d = nn.LazyConv2d(1, kernel_size = 3, padding = 1)\n",
        "\n",
        "X = torch.rand(size = (8, 8))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thAIgP66WiQl",
        "outputId": "c1c138b9-e540-4afc-8238-92524ee34106"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a convolution layer with different height and width but selecting different padding to get the same output shape\n",
        "\n",
        "conv2d = nn.LazyConv2d(1, kernel_size = (5, 3), padding = (2, 1))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2MqDvt9YFoU",
        "outputId": "b2719359-97eb-47b4-ed93-c98144c0e941"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stride \n",
        "\n",
        "For computational efficiency, we might want to slide the conv layer more than 1 step at a time, skipping the intermediate locations. \n",
        "\n",
        "Number of rows and columns per traversed per slide is called stride. SO far we used strides 1. \n",
        "\n",
        "If the stride height is sh, stride width is sw, the output shape is \n",
        "\n",
        "$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$\n",
        "\n",
        "If we set ph = kh - 1 and pw = kw - 1, the output shape is simplified to \n",
        "\n",
        "$\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$. "
      ],
      "metadata": {
        "id": "ZkuCm_bRYghD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size = 3, padding = 1, stride = 2)\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqHsVfv6bGcJ",
        "outputId": "55d538d3-3693-4bfa-9fc7-41fa19aa2d1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size = (3, 5), padding = (0, 1), stride = (3, 4))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyKVNZvubMtQ",
        "outputId": "d4c58a83-ea9f-4698-bbc7-9822dcfde728"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Input and Multiple Output Channels\n",
        "\n",
        "Notion of channels is as old as CNNs themselves (Since 1995 almost)\n",
        "\n",
        "### Multiple Input Channels\n",
        "\n",
        "The convolution kernel must have the same number of channels as the input layer. The outputs of the kernel at each channel are then added that result in a single channel kernel output. \n"
      ],
      "metadata": {
        "id": "J4qlrD2UbV7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def corr2d_multi_channel(X, K):\n",
        "  # Iterate through each dimension of K (each channel of K) then add them\n",
        "  return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
      ],
      "metadata": {
        "id": "xyWaRRhqefLW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
        "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
        "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
        "\n",
        "corr2d_multi_channel(X, K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neLDoa9XezFj",
        "outputId": "61f0deec-73b7-4791-c1d4-3757ce487b7e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  72.],\n",
              "        [104., 120.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Output Channels\n",
        "\n",
        "In the above example, no matter the number nof inputs, we end up getting a single dimension output. But it is essential to have multiple channel outputs. In most popular neural networks we add more channels to the output as we go in depth into the neural network architecture. This is done to compensate for the lack of spatial resolution. Intuitively, this can be thought of as each channel corresponding to a different set of features. \n",
        "\n",
        "If ci and co are the number of input and output channels, kh and kw are kernel height and width, to get multi channel kernel output we create kernel of shape ci x kh x kw for each output channel co. We concatenate them on the output channel dimension so that shape is co x ci x kh x kw. "
      ],
      "metadata": {
        "id": "K-p8Vwkve4TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def corr2d_multi_channel_kernel_output(X, K):\n",
        "  # Iterate through the 0th dimension of 'K' and each time perform \n",
        "  # cross correlation operations with input 'X'. All of the results are stacked\n",
        "  # together\n",
        "  return torch.stack([corr2d_multi_channel(X, k) for k in K], 0)\n"
      ],
      "metadata": {
        "id": "vJz5Qt-6f34b"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = torch.stack((K, K+1, K+2), 0)\n",
        "K.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MjaQ-NTgPOP",
        "outputId": "2ec02d3a-2c70-41ed-8ca1-85c5c9ea1625"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr2d_multi_channel_kernel_output(X, K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc6-cpeBgUXU",
        "outputId": "8d4c1bf2-42e1-423f-a039-4fddbd7bd019"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 56.,  72.],\n",
              "         [104., 120.]],\n",
              "\n",
              "        [[ 76., 100.],\n",
              "         [148., 172.]],\n",
              "\n",
              "        [[ 96., 128.],\n",
              "         [192., 224.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1x1 Convolutional Layer\n",
        "\n",
        "Because of it's size a 1x1 kernel cannot capture the patterns or interactions between adjacent elements in height and width dimensions. It's only computational benefit is to work on the channel dimension.\n",
        "\n",
        "It can be used to change the channele dimension from ci to co. "
      ],
      "metadata": {
        "id": "T0OwyB51gaPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def corr2d_multi_in_out_1x1(X, K):\n",
        "  c_i, h, w = X.shape\n",
        "  c_o = K.shape[0]\n",
        "  X = X.reshape((c_i, h * w))\n",
        "  K = K.reshape((c_o, c_i))\n",
        "  # Matrix Multiplication in the fully connected layer\n",
        "  Y = torch.matmul(K, X)\n",
        "  return Y.reshape((c_o, h, w))"
      ],
      "metadata": {
        "id": "geQ_y22djTfd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.normal(0, 1, (3, 3, 3))\n",
        "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
        "\n",
        "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
        "Y2 = corr2d_multi_channel_kernel_output(X, K)\n",
        "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"
      ],
      "metadata": {
        "id": "6hc_C80YjnyC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Channels allow us to combine MLPs that allow for significant non linearities and Convolutions that allow for localized analysis of features. Channels allow CNN to reason with multiple features like edge and shape detection at the same time. They also reduce the number of parameters as compared to regular MLPs. \n",
        "\n",
        "Although this flexibility comes at a price. An image of size (h x w) the cost of computing k x k convolution is O(h x w x k\\*\\*2). For ci and co channel sizes, it becomes O(h x 2 x k\\*\\*2 x ci x co)"
      ],
      "metadata": {
        "id": "cyACxIQfjs8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling\n",
        "\n",
        "The deeper we go into the network, the larger the receptive field (size relative to the input) becomes to wihch each node is sensitive. Reducing spatial resolution accelerates this process. \n",
        "\n",
        "Pooling layers serve the dual purposes of mitigating sensitivity of con layers to location and spatially downsampling representations. \n",
        "\n",
        "### Maximum Pooling and Average Pooling\n",
        "\n",
        "Like conv operators, pooling operators consist of fixed shape window that is slid over all regions in the input according to its stride, computing a single value for each location traversed by the window. Unlike conv operations, pooling has no parameters. Pooling operations are deterministic ie., they calculate average or maximum of all elements in the pooling window. These are called average pooling or maximum pooling. \n",
        "\n",
        "Average pooling is similar to downsampling an image. "
      ],
      "metadata": {
        "id": "mcF_XLVikvHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def pool2d(X, pool_size, mode = 'max'):\n",
        "  p_h, p_w = pool_size\n",
        "  Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
        "  for i in range(Y.shape[0]):\n",
        "    for j in range(Y.shape[1]):\n",
        "      if mode == 'max':\n",
        "        Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
        "      elif mode == 'avg':\n",
        "        Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
        "  return Y"
      ],
      "metadata": {
        "id": "wgi2p5HJzqTf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "pool2d(X, (2, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu5oAzGMBWgs",
        "outputId": "eb52e46e-4bca-4d18-a526-cd9d0a08f23f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5.],\n",
              "        [7., 8.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pool2d(X, (2,2), 'avg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nADRbIKjB18o",
        "outputId": "e4ebc567-6375-400a-b0ab-4c2e2fbedc70"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 3.],\n",
              "        [5., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding and Stride\n",
        "\n",
        "Similar to conv layers, pooling layers alter the output shape. We can also use pooling and striding to control this change in the output shape. "
      ],
      "metadata": {
        "id": "sK8aFSs9DPHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(16, dtype = torch.float32).reshape((1, 1, 4, 4))\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2pJnWJxDyps",
        "outputId": "a266d22f-727e-42e5-d37b-86053942920d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  1.,  2.,  3.],\n",
              "          [ 4.,  5.,  6.,  7.],\n",
              "          [ 8.,  9., 10., 11.],\n",
              "          [12., 13., 14., 15.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pool2d = nn.MaxPool2d(3)\n",
        "pool2d(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVVyHg1ND71J",
        "outputId": "f94fedfa-8e4a-4f71-c038-caa71062d9f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[10.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pool2d = nn.MaxPool2d(3, padding = 1, stride = 2)\n",
        "pool2d(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRM0gGKqD_v2",
        "outputId": "d4c9d2dc-6296-47bc-eb36-adcdd9481f84"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 5.,  7.],\n",
              "          [13., 15.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pool2d = nn.MaxPool2d((2, 3), stride = (2, 3), padding = (0, 1))\n",
        "pool2d(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHeszQfnEHCj",
        "outputId": "170e27f0-155c-4a95-d661-91f17556ebee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 5.,  7.],\n",
              "          [13., 15.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Channels\n",
        "\n",
        "When dealing with inputs  with multiple channels, pooling layer pools each channel seperately unlike conv layer that just adds them together. This means the number of input and output channels are going to be the same. "
      ],
      "metadata": {
        "id": "nr88wT_rEVDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.cat((X, X + 1), 1)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsuVzljtEpU-",
        "outputId": "378eefa2-fb3b-4a5d-f0d0-ed22972ddf38"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  1.,  2.,  3.],\n",
              "          [ 4.,  5.,  6.,  7.],\n",
              "          [ 8.,  9., 10., 11.],\n",
              "          [12., 13., 14., 15.]],\n",
              "\n",
              "         [[ 1.,  2.,  3.,  4.],\n",
              "          [ 5.,  6.,  7.,  8.],\n",
              "          [ 9., 10., 11., 12.],\n",
              "          [13., 14., 15., 16.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pool2d = nn.MaxPool2d(3, padding = 1, stride = 2)\n",
        "pool2d(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WEI7nqaEuk7",
        "outputId": "10fbf1a2-6b07-4177-fbd7-90afb2feaece"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 5.,  7.],\n",
              "          [13., 15.]],\n",
              "\n",
              "         [[ 6.,  8.],\n",
              "          [14., 16.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the layer has sparse fetures with sharp regions, max pooling will likely retain the patterns, whereas average pooling will simply blur them. "
      ],
      "metadata": {
        "id": "TjckFt5AE9Df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Networks (LeNet)\n",
        "\n",
        "LeNet is one of the first published CNNs to capture wide attention for its performance on CV tasks. \n",
        "\n",
        "### LeNet\n",
        "\n",
        "It consists of two parts. \n",
        "\n",
        "1 - A convolutional encoder consisiting of two convolutional layers\n",
        "\n",
        "2 - A dense block consisting of three fully connected layers. \n",
        "\n",
        "The basic units in each convlution block are a convolutional layer, a sigmoid activateion function and a subsequent average pooling operation. \n",
        "\n",
        "ReLUs and Max Pooling have not been used because they havent been discovered at that time. \n",
        "\n",
        "Each Convolution Layer uses a 5x5 kernel. The number of channels are increased in each block. In the first conv layer, there are 6 output channels and the second has 16. Each 2x2 pooling operation with stride 2 reduces the dimensionality by a factor of 4 via spatial downsampling. \n",
        "\n",
        "Before passing the output of the second conv layer to the dense block, the output is flattened by converting it from 4 dimensions to 2 dimensions. LeNet's dense block has three fully connected layers with 120, 84, 10 outputs respectively. "
      ],
      "metadata": {
        "id": "ERPrn2iXFTz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        " \n",
        "# def int_cnn(module):\n",
        "#   if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
        "#     nn.init.xavier_uniform(module.weight)\n",
        "\n",
        "# class LeNet():\n",
        "#   def __init__(self, lr = 0.1, num_classes = 10):\n",
        "#     super().__init__()\n",
        "#     self.net = nn.Sequential(\n",
        "#         nn.LazyConv2d(6, kernel_size = 5, padding = 2), nn.Sigmoid(),\n",
        "#         nn.AvgPool2d(kernel_size = 2, stride = 2),\n",
        "#         nn.LazyConv2d(16, kernel_size = 5), nn.Sigmoid(),\n",
        "#         nn.AvgPool2d(kernel_size = 2, stride = 2),\n",
        "#         nn.Flatten(),\n",
        "#         nn.LazyLinear(120), nn.Sigmoid(),\n",
        "#         nn.LazyLinear(84), nn.Sigmoid(),\n",
        "#         nn.LazyLinear(num_classes)\n",
        "#     )\n",
        "\n",
        "net = nn.Sequential(\n",
        "  nn.Conv2d(1, 6, kernel_size = 5, padding = 2), nn.Sigmoid(),\n",
        "  nn.AvgPool2d(kernel_size = 2, stride = 2),\n",
        "  nn.Conv2d(6, 16, kernel_size = 5), nn.Sigmoid(),\n",
        "  nn.AvgPool2d(kernel_size = 2, stride = 2),\n",
        "  nn.Flatten(),\n",
        "  nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
        "  nn.Linear(120, 84), nn.Sigmoid(),\n",
        "  nn.Linear(84, 10)\n",
        ")"
      ],
      "metadata": {
        "id": "z8C3_TXjKmle"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(size = (1, 1, 28, 28), dtype = torch.float32)\n",
        "for layer in net:\n",
        "  X = layer(X)\n",
        "  print(layer.__class__.__name__, 'output shape: \\t', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4z96mxrLgl3",
        "outputId": "49f0963c-9bbe-47cd-aa0c-1ff19e59a6a3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d output shape: \t torch.Size([1, 6, 28, 28])\n",
            "Sigmoid output shape: \t torch.Size([1, 6, 28, 28])\n",
            "AvgPool2d output shape: \t torch.Size([1, 6, 14, 14])\n",
            "Conv2d output shape: \t torch.Size([1, 16, 10, 10])\n",
            "Sigmoid output shape: \t torch.Size([1, 16, 10, 10])\n",
            "AvgPool2d output shape: \t torch.Size([1, 16, 5, 5])\n",
            "Flatten output shape: \t torch.Size([1, 400])\n",
            "Linear output shape: \t torch.Size([1, 120])\n",
            "Sigmoid output shape: \t torch.Size([1, 120])\n",
            "Linear output shape: \t torch.Size([1, 84])\n",
            "Sigmoid output shape: \t torch.Size([1, 84])\n",
            "Linear output shape: \t torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size = batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477,
          "referenced_widgets": [
            "0abea00cd4a24136a4e836ac0da20153",
            "1b0de0d0bdfd483c92bfb470b38e31fb",
            "222b200164f2452aa3e05ade07452bc3",
            "22d270a850e54373a93b3481e7eef240",
            "4dcf6bf037414bc984560b0ec25f0a92",
            "4705184402d842b5b6877735459e7bec",
            "c2c07779d9aa41f8bf5d9e10ace7e8ff",
            "24634db0f5464613a80fca5e4cbba3c6",
            "6da76f94ee00470a87c1a2a3aa44a942",
            "5a6ce7131a4f4274b697bd818e034983",
            "af46a82fb3f04207a8c5196c231068b5",
            "223e59734a1242da9d6c43d04972b4dc",
            "0e390176548d4c67b0d3e59e8d51e6aa",
            "7c4891aedf9a4a49aee2b46802a84d7e",
            "b5e1a423a432423086a1793887a7ec79",
            "83a99d6726ea49249d30fdae90e47d76",
            "ee668d4b5a894e8699658bfbd757c230",
            "74ecb840b18a4a6fa051798a669b47e8",
            "29aa70c2a43640acad4969cca40c6d8b",
            "9f0f9a08f3a343bb9ccda6ca3d4b266e",
            "352fa135387544f69d492dadf67e4f65",
            "f0efabcc66a749c2b067f366ebb5750f",
            "160cd30044754d29af5ad63f423a58e6",
            "c63a0e2cc9404454ade4708c41ee6179",
            "d9f85803fea94a2e80880e76e2d2bf51",
            "c3547e3e1e2b4f3cac9b30c0ac3381b7",
            "ac47bb7b8df84b01ba77b68117c4d9fa",
            "46a89b9d41f74ab88791977a81db80bf",
            "b13904f7d3d94b82b0b642507faab214",
            "507ca77ce1104125ac14be00de1244d0",
            "af5d8696c72e4acea447dad96c9e45bb",
            "b85a8815f5cb438baa25bd4cdf754001",
            "f4f89ea24db04c1d927fde8e54638d6b",
            "a2c01048c3f041a6a3e48fd764542ee5",
            "d59b8d9073734afaa74df03de1062403",
            "6f85b3b8c00e424c93218f791b47c0d8",
            "c84413b91df14caf8a8773f8eb308725",
            "af9a59eb65ef449595ba3aecbcd55a76",
            "ed4778178d884b2484078c576ded399c",
            "e57f16665ef2425fbe1a2730de32963f",
            "d60ea21eccd4402394c1d1f73783778f",
            "80229366c7074a15a8e509387eb48f8e",
            "95d20b3036064733acb0b38e6c107fdc",
            "3575cb9289744bd2a4fa3af248229711"
          ]
        },
        "id": "vldk1el5MCQ-",
        "outputId": "0e0185d5-f81f-4e77-9fee-09969aefb570"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/26421880 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0abea00cd4a24136a4e836ac0da20153"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/29515 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "223e59734a1242da9d6c43d04972b4dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4422102 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "160cd30044754d29af5ad63f423a58e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2c01048c3f041a6a3e48fd764542ee5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy_gpu(net, data_iter, device = None):\n",
        "  if isinstance(net, nn.Module):\n",
        "    net.eval()\n",
        "    if not device:\n",
        "      device = next(iter(net.parameters())).device\n",
        "    metric = d2l.Accumulator(2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for X, y in data_iter:\n",
        "        if isinstance(X, list):\n",
        "          X = [x.to(device) for x in X]\n",
        "        else:\n",
        "          X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        metric.add(d2l.accuracy(net(X), y), y.numel())\n",
        "  return metric[0]/metric[1]"
      ],
      "metadata": {
        "id": "piircYkISck6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0bjk8X8dTxbC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}