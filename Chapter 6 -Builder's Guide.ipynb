{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 6 - Builder's Guide.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOmLjn4sS1Q+Cn0MGilzx3y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asrjy/d2l-notes/blob/master/Chapter%206%20-Builder's%20Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layers and Modules\n"
      ],
      "metadata": {
        "id": "jJ94vtqyy_fq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1w_1_FLpq3K",
        "outputId": "3fb80253-4844-4dba-8c84-1a415ad42e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn \n",
        "from torch.nn import functional as F\n",
        "\n",
        "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
        "\n",
        "X = torch.rand(2, 20)\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.__call__(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvWQQTMmqAFZ",
        "outputId": "8383da76-edc0-40e4-9232-39c23fdb7c83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Custom Module\n",
        "\n",
        "Basic functionality of a module:\n",
        "\n",
        "1 - Ingest input data as arguments and pass it to it's forward propagation method. \n",
        "\n",
        "2 - Generate an output from the input passed to it, at the end of forward propagation computation. \n",
        "\n",
        "3 - Calculate the backpropagation of the output with respect to the input. \n",
        "\n",
        "4 - Store and provide access to it's parameters that are necessary for the forward propagation (weights). \n",
        "\n",
        "5 - Initialize model parameters as needed. "
      ],
      "metadata": {
        "id": "POZM0yUNqTBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    # Calling the constructor of the parent class nn.Module to perform the necessary initialization\n",
        "    super().__init__()\n",
        "    self.hidden = nn.LazyLinear(256)\n",
        "    self.out = nn.LazyLinear(10)\n",
        "  def forward(self, X):\n",
        "    return self.out(F.relu(self.hidden(X)))"
      ],
      "metadata": {
        "id": "L50PjFkZtYXL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = MLP()\n",
        "net(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksIFKXZBuRfb",
        "outputId": "97863fdf-994d-47c0-ae8d-5bf23a4a4957"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Sequential Module\n",
        "\n",
        "We can build our own version of Sequential if we can provide \n",
        "\n",
        "1 - A method to append modules one by one to  a list\n",
        "\n",
        "2 - A forward propagation method to pass an input through a chain of modules"
      ],
      "metadata": {
        "id": "BAyWc8pouUq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MySequential(nn.Module):\n",
        "  def __init__(self, *args):\n",
        "    super().__init__()\n",
        "    for idx, module  in enumerate(args):\n",
        "      self.add_module(str(idx), module)\n",
        "    \n",
        "  def forward(self, X):\n",
        "    for module in self.children():\n",
        "      X = module(X)\n",
        "    return X"
      ],
      "metadata": {
        "id": "nfcAIY10v8wx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
        "net(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgC5bKF7wJmn",
        "outputId": "b558f51e-5bf6-4627-c126-df7b6dbc43f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executing code in the forward propagation method\n",
        "\n",
        "Sequential() is not so helpful when we want to include python control flow during forward propagation or apply some mathematical operations on the output of layers instead of relying on predefined network layers. \n",
        "\n",
        "We may also use constant parameters that are not a result of previous iteration or are updatable parameters. \n",
        "\n",
        "Defining an MLP that does this\n"
      ],
      "metadata": {
        "id": "wGJWLYUAxcP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedHiddenMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.rand_weight = torch.rand((20, 20))\n",
        "    self.linear = nn.LazyLinear(20)\n",
        "  def forward(self, X):\n",
        "    X = self.linear(X)\n",
        "    X = F.relu(X @ self.rand_weight + 1)\n",
        "    # Reusing the fully connected layer. This is equivalent to sharing parameters with two fully connected layers\n",
        "    X = self.linear(X)\n",
        "    # This may not be seen in a real life neural network. Just to showcase the advantage of creating a custom class instead of using Sequential() class. \n",
        "    while X.abs().sum() > 1:\n",
        "      X /= 2\n",
        "    return X.sum()"
      ],
      "metadata": {
        "id": "JzQWQNmTyDPX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = FixedHiddenMLP()\n",
        "net(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnvW700lyjjF",
        "outputId": "4fd82c81-1cba-4714-937f-e2309b178392"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0711, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use Sequntial inside of class in other words nesting of modules is possible. "
      ],
      "metadata": {
        "id": "W3VSpqhay3TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NestMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(), nn.LazyLinear(32), nn.ReLU())\n",
        "    self.linear = nn.LazyLinear(16)\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.linear(self.net(X))"
      ],
      "metadata": {
        "id": "tzHKatn_zNMI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
        "chimera(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLqztEBbzgSF",
        "outputId": "b2ca086b-9bcd-4921-bcf1-5bff24cb92e1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.3221, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Management"
      ],
      "metadata": {
        "id": "NotCDy95zl3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes we may need to access the parameters of the model. Either to store them in the disk, or when we are working with a complex model and don't want to leave the initialization to the library, or any similar reasons. "
      ],
      "metadata": {
        "id": "FbXHpZ5s1CjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\n",
        "X = torch.rand(size = (2, 4))\n",
        "net(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG_TkER111UK",
        "outputId": "100f0c25-8517-4146-d428-e2e907ecc5d3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter Access\n",
        "\n",
        "Each layer's attributes are available to be accessed in it's corresponding attribute. "
      ],
      "metadata": {
        "id": "EfX-cFqV2AS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net[2].state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2at1-r6B2C6U",
        "outputId": "1059c55a-a095-42e0-d77e-ed772d13e047"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[ 0.0707,  0.0520, -0.2712, -0.2416, -0.2818, -0.3370,  0.2806, -0.0592]])),\n",
              "             ('bias', tensor([-0.1263]))])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Targeted Parameters\n",
        "\n",
        "Parameters are complex objects containing values, gradients and additional information. When requested, PyTorch returns a parameter object. So we need to request the data explicityly if we need to access the underlying numerical values. "
      ],
      "metadata": {
        "id": "LIHo8THY2OTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(net[2].weight), net[2].weight.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi9BCZSS2ocQ",
        "outputId": "89c0759a-ec8d-4838-9f14-34bde1199328"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.nn.parameter.Parameter,\n",
              " tensor([[ 0.0707,  0.0520, -0.2712, -0.2416, -0.2818, -0.3370,  0.2806, -0.0592]]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this network's backpropagation has not been initiated yet, grad should be a None value. "
      ],
      "metadata": {
        "id": "w_RL5qAP2s_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net[2].bias.grad == None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-jQvWZ_23o9",
        "outputId": "8442a233-5cb7-4384-d28d-a45b9a01b2bb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All parameters at once"
      ],
      "metadata": {
        "id": "RBuzHWhd26HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(name, param.shape) for name, param in net.named_parameters()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-foXVjh2-Wr",
        "outputId": "6fae12c9-f526-4181-c4d8-5da1ad42e88a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('0.weight', torch.Size([8, 4])),\n",
              " ('0.bias', torch.Size([8])),\n",
              " ('2.weight', torch.Size([1, 8])),\n",
              " ('2.bias', torch.Size([1]))]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tied Parameters/Weight Sharing"
      ],
      "metadata": {
        "id": "YEJLdqZ_3E8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shared = nn.LazyLinear(8)\n",
        "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.LazyLinear(1))\n",
        "net(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InmOQWvC3NkH",
        "outputId": "44a601bb-7b43-4fad-f43a-74a6605e4bdd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0531],\n",
              "        [-0.0540]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
        "net[2].weight.data[0, 0] == 100\n",
        "# Since they are the same object, changing at one place will be reflected on the other side\n",
        "print(net[2].weight.data[0] == net[4].weight.data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeX3TQBA5Alh",
        "outputId": "873c4642-2c27-4de5-c255-5c720103d4cc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True, True, True, True, True, True, True])\n",
            "tensor([True, True, True, True, True, True, True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since they are shared, the gradients are also added during backpropagation"
      ],
      "metadata": {
        "id": "W6gSE7EK5OUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Hax4RosS5kNE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}