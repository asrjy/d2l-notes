{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 7 - Convolutional Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBmRG3uQSkh2bmIZwqwQTQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asrjy/d2l-notes/blob/master/Chapter%207%20-%20Convolutional%20Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "Flattening image layers means losing the spatial relationship between the pixels. In order to overcome this issue, we use CNNs instead of regular MLPs\n",
        "\n",
        "Since MLPs don't care about the spatial relationship, we get the same results even though the order of the pixels are modified. This phenomenon is not wanted. \n",
        "\n",
        "Modern CNNs tend to be computationally effieicent, require fewer parameters than fully connected networks and easy to parallelize over multiple GPU cores. CNNs have also shown decent performance on one dimenstionals data such as audio, time series data where recurrent neural networks are conventionally used. \n",
        "\n",
        "## From Fully Connected Layers to Convolutions\n",
        "\n",
        "For low dimensional data where we lack the knowledge to construct specific architectures that identify patterns that seek interactions among features, an MLP may be the best we can do. However, for high dimensional perceptual data, such structure less networks can gro unwieldy. \n",
        "\n",
        "Convolutional Neural Networks are one way to exploit the structure in natural images. \n",
        "\n",
        "### Invariance\n",
        "\n",
        "Spatial Invariance allows CNNs to learn useful representations with fewer parameters. What it basically means is our model should identify a pig in air or a plane in water. Where the target is located is not of importance, it's existence is what we look for. \n",
        "\n",
        "There are two different types of invariances. \n",
        "\n",
        "1 - In the earliest layers, our network should respond similarly to the same patch, regardless of where the patch is located in the image. A patch here means a part of the image on which the NN works on. This principle is called translation invariance / translation equivariance. \n",
        "\n",
        "2 - The earliest layers of the network should focus on local regions, without regard for the contents in distant regions. This is called locality principle. Eventually they are aggregated to make predictions on the image as a whole. \n",
        "\n",
        "3 - As we proceed, deeper layers should capture longer range features of the image similar to higher level vision in nature. \n",
        "\n",
        "### Constraining the MLP\n",
        "\n",
        "Assume the image is represented as X with shape (i, j) and the hidden representation is represented as H with shape also (i, j) where each element of the hidden representation is calculated by summing over the pixels of X centered at i, j weighted by V (i, j, a, b)\n",
        "\n",
        "\\begin{split}\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l}\\\\ &=  [\\mathbf{U}]_{i, j} +\n",
        "\\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b}  [\\mathbf{X}]_{i+a, j+b}.\\end{aligned}\\end{split}\n",
        "\n",
        "The indices a, b run over both positive and negative offsets ocvering the entire image. \n",
        "\n",
        "#### Translation Invariance\n",
        "\n",
        "According to this principle, a shift in X should lead to a shift in the hidden representation. This is only possible if V and U do not depepnd on i, j. As a result we represent V(i, j, a, b) as just V((a, b) and U as a constant. \n",
        "\n",
        "Now the simplified representation is \n",
        "\n",
        "$[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}$.\n",
        "\n",
        "The above concept is called convolution where we are effectively weighing pixels at (i+a j+b) in the vicinity of (i, j) with coefficients V(a, b) to obtain the hidden representation. \n",
        "\n",
        "V(a, b) requires far fewer parameters than V(i, j, a, b) since it no longer depends on i and j. \n",
        "\n",
        "#### Locality\n",
        "\n",
        "According to this principle, we should not look very far from X(i, j) to get relevant information about what is going on at X(i, j). This means, after some value of a and b, V(a, b) need to be 0. \n",
        "\n",
        "Now the hidden representation becomes, \n",
        "\n",
        "$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}$.\n",
        "\n",
        "The value of delta is typically smaller than 10. \n",
        "\n",
        "The above equation is called a convolutional layer. V is referred to as Convolution kernel/filter. \n",
        "\n",
        "Without this layer, for a single megeapixel image, we would require billions of parameters, but with the convolutional layer, we would require a few hundred, without altering the dimensionality of either the inputs or the hidden representations. \n",
        "\n",
        "The cost of this reduction of in parameters is that, we only capture local information while determining the value of each hidden activation. \n",
        "\n",
        "This bias might not always agree with reality, as there could be images that are not translation invariant. \n",
        "\n",
        "### Convolutions\n",
        "\n",
        "In mathematics, a convolution operation between two functions is the measure of overlap between f and g when g is flipped and shifted by x. \n",
        "\n",
        "$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}.$\n",
        "\n",
        "When we are dealing with discrete objects, the integral in the beginning, becomes a sum. \n",
        "\n",
        "$(f * g)(i) = \\sum_a f(a) g(i-a).$\n",
        "\n",
        "For two dimensional tensors, we have corresponding indices a and b for i and j. \n",
        "\n",
        "$(f * g)(i, j) = \\sum_a\\sum_b f(a, b) g(i-a, j-b).$\n",
        "\n",
        "This is similar to the convolution operation we arrived at before excluding the + instead of -. The more proper name for the equation we got before is cross-correlation. \n",
        "\n",
        "### Channels\n",
        "\n",
        "To support multiple channels in both the inputs and the hidden representations, we add two more coordinates to V and one more to X. \n",
        "\n",
        "$[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c},$\n",
        "\n",
        "where d indexes the output channels in the hidden representation. \n",
        "\n",
        "### Exercises \n",
        "\n",
        "1 - Assume that the size of the convolution kernel is $\\Delta = 0$. Show that in this case the convolution kernel implements an MLP independently for each set of channels. This leads to the Network in Network architectures [Lin et al., 2013].\n",
        "\n",
        "Ans - When the size of convolution layer is 0, it means, nearby data of each coordinate is not considered. There's no convolution happening. The shape of V and X are going to be the same. This is what happens in a regular MLP. \n",
        "\n",
        "2.1 - Audio data is often represented as a one-dimensional sequence. When might you want to impose locality and translation invariance for audio?\n",
        "\n",
        "Ans - Inclusion of locality and translation invariance means, audio at one part is not necessarily related to audio at another part. This wouldn't work for speech related problems, but would work for pattern recognition problems where local part doesnt have any relation to parts outside its locality.\n",
        "\n",
        "3 - Why might translation invariance not be a good idea after all? Give an example.\n",
        "\n",
        "Ans - Not a good idea in cases where one patch of the image has a relationship with it's position. An example could be when identifying the face of a human, if the position of eyes and mouth are exchanged, the prediction probably would not be a human. \n",
        "\n",
        "4 - Do you think that convolutional layers might also be applicable for text data? Which problems might you encounter with language?\n",
        "\n",
        "Ans - Convolutional Layers include translational invariance and Locality. Translation Invariance would not work on text data as it is sequential data. Locality would not be of much help since words make sense only when combined with other words. \n",
        "\n",
        "5 - What happens with convolutions when an object is at the boundary of an image.\n",
        "\n",
        "Ans - Values around the border are decided using padding. \n",
        "\n",
        "## Convolutions for Images\n",
        "\n",
        "### The Cross-Correlation Operation\n",
        "\n",
        "If the input is of size n_h \\times n_w and the kernel is of size $k_h \\times k_w$, the output after the cross correlation operation would be of size $(n_h-k_h+1) \\times (n_w-k_w+1).$\n",
        "\n",
        "This is because we need enough space to shift the kernel across the image. With padding, the output size will not be varied as we pad zeros around the boundary so there is enough space to shift the kernel. "
      ],
      "metadata": {
        "id": "0edxHfhNcibH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zniQ-nzn9pBs",
        "outputId": "66ca12d1-3d85-4865-ba95-6bb5ed6089c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting d2l\n",
            "  Downloading d2l-0.17.5-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 732 kB/s \n",
            "\u001b[?25hCollecting pandas==1.2.4\n",
            "  Downloading pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 11.4 MB/s \n",
            "\u001b[?25hCollecting numpy==1.21.5\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 8.9 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.5.1\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 16.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0)\n",
            "Collecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.2.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (4.10.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.3.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (7.7.0)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.34.2-py3-none-any.whl (944 kB)\n",
            "\u001b[K     |████████████████████████████████| 944 kB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (1.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4->d2l) (2022.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.5.1->d2l) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1->d2l) (1.15.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.3.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.2.5)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (5.4.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (1.1.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (5.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.11.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (3.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter==1.0.0->d2l) (23.1.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter==1.0.0->d2l) (2.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.0.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter==1.0.0->d2l) (2.1.0)\n",
            "Installing collected packages: numpy, fonttools, requests, pandas, matplotlib, d2l\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.25.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed d2l-0.17.5 fonttools-4.34.2 matplotlib-3.5.1 numpy-1.21.5 pandas-1.2.4 requests-2.25.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def corr2d(X, K):\n",
        "  h, w = K.shape\n",
        "  Y = torch.zeros((X.shape[0] - h + 1) , (X.shape[1] - w + 1))\n",
        "  for i in range(Y.shape[0]):\n",
        "    for j in range(Y.shape[1]):\n",
        "      Y[i, j] = (X[i: i+h, j:j+w] * K).sum()\n",
        "  return Y "
      ],
      "metadata": {
        "id": "iclTf9TSci_T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "corr2d(X, K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkDanouF-LHt",
        "outputId": "eac53867-b660-4645-bb62-92b6f5ee3b5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[19., 25.],\n",
              "        [37., 43.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Layers\n",
        "\n",
        "A convolutional layer performs cross correlation  between input and kernel and adds a scalar bias to it to produce am output. The two parameters of a convolutional layer are the kernel and the bias. When training, we typically intialize them randomly. \n",
        "\n",
        "Implementing a convolutional layer based on the corr2d function defined above. "
      ],
      "metadata": {
        "id": "WsuHbdod-dKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2D(nn.Module):\n",
        "  def __init__(self, kernel_size):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.rand(kernel_size))\n",
        "    self.bias = nn.Parameter(torch.zeros(1))\n",
        "  def forward(self, X):\n",
        "    return corr2d(X, self.weight) + self.bias"
      ],
      "metadata": {
        "id": "DqtXgXK4B0kt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Edge Detection in Images\n",
        "\n",
        "Detecting the edge of an object can be performed by finding the location of change of color in piels. "
      ],
      "metadata": {
        "id": "uo0Xd6nNCQb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.ones((6, 8))\n",
        "X[:, 2:6] = 0\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmPsVBgICeZu",
        "outputId": "1d540661-20d9-406a-8b7a-36adabd01fca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we perform cross correlation with a kernel of size 1x2, if the two elements match with the elements in the patch of the image, it outputs 0 ie., at location i, j it calculates x(i, j) - x(i+1, j)"
      ],
      "metadata": {
        "id": "TJuqUi1OCkBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = torch.tensor([[1.0, -1.0]])"
      ],
      "metadata": {
        "id": "J85n9at2DfA6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = corr2d(X, K)\n",
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l64kHQzIDvgd",
        "outputId": "19e4f91d-fea6-4a8b-cf25-bdb0220dfd4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr2d(X.t(), K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAfeaR0QDy__",
        "outputId": "32e64492-19d2-4a70-e563-420105014af3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does not work because, the cross correlation can only detect vertical edge, not horizontal edges. \n",
        "\n",
        "### Learning a Kernel\n",
        "\n",
        "Although the above process is nice, it's hard to define each kernel when we are working with bigger architectures. We want to kernel to automatically learn these processes. \n",
        "\n",
        "First we construct a convolution layer and initialize it randomly. In each iteration we will use the squared error to compare Y with the output of the convolution layer, calculate the gradient to update the kernel. "
      ],
      "metadata": {
        "id": "hbci4EDoECar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional Layer with 1 output channel and kernel fo shape (1, 2). Ignoring the bias for now. \n",
        "conv2d = nn.LazyConv2d(1, kernel_size = (1, 2), bias = False)\n",
        "\n",
        "# (Example, Channel, Height, Width)\n",
        "X = X.reshape((1, 1, 6, 8))\n",
        "Y = Y.reshape((1, 1, 6, 7))\n",
        "lr = 3e-2\n",
        "\n",
        "for i in range(10):\n",
        "  Y_hat = conv2d(X)\n",
        "  l = (Y_hat - Y) ** 2\n",
        "  conv2d.zero_grad()\n",
        "  l.sum().backward()\n",
        "  # Updating the kernel\n",
        "  conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
        "  if (i+1)%2 == 0:\n",
        "    print(f\"Epoch {i+1}, loss {l.sum():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM20r8_YEMbU",
        "outputId": "47283bbf-9d82-41be-96c1-4ddaf7c9c579"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, loss 11.266\n",
            "Epoch 4, loss 2.136\n",
            "Epoch 6, loss 0.459\n",
            "Epoch 8, loss 0.118\n",
            "Epoch 10, loss 0.037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d.weight.data.reshape((1, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRAH1M0gHICj",
        "outputId": "fa5e544f-2f12-4439-a721-6a5055926cd9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0008, -0.9659]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Correlation and Convolution\n",
        "\n",
        "In order to perform just convolution instead of cross correlation, we need to flip the kernel both horizontally and vertically, then perform cross correlation with the input tensor. \n",
        "\n",
        "Since kernels are learnt from data, it doesnt matter whether layers perform cross correlation or convolution. The output remains the same. \n",
        "\n",
        "Meaning if a layer performs cross correlation and it's weights are represented as K, the learned kernel be K', K' will be the same even when K is flipped horizontally and vertically. \n",
        "\n",
        "### Feature Map and Receptive Field\n",
        "\n",
        "The outpt of Convolution Layer is sometimes called Feature Map. Receptive field of any element x of any layer means the elements from the previous layers, that cal affect the calculation of x during the forward propagation. It may be larger than the actual size of the input. "
      ],
      "metadata": {
        "id": "90BFBg_PLyv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding and Stride \n",
        "\n",
        "10 Convolution Layers of kernel size 5x5 on a 240x240 image, reduce the output size to 200x200. Padding and Strided Convolutions offer more control over the size of the output. \n",
        "\n",
        "### Padding\n",
        "\n",
        "One straightforward solution to this issue is to add zeros around the image. If we add ph rows of padding (half on top and the rest on bottom) and pw columns of padding (half on left and the rest on right), the output shape would be \n",
        "\n",
        "(nh - kh + ph + 1) x (nw - kw + pw + 1). \n",
        "\n",
        "In many cases ph = kh - 1 and pw = kw - 1 to give input and output the same height and width. \n",
        "\n",
        "CNNs commonly use conv kernels of odd height and width. This means when we pad, we can divide the number of rows and columns by 2, thus having equal row and column paddings on both sides. \n"
      ],
      "metadata": {
        "id": "wUtCkMtgUA7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def comp_conv2d(conv2d, X):\n",
        "  # conv2d requires images of 4 dimensions. Adding the example and channel dimension\n",
        "  X = X.reshape((1, 1) + X.shape)\n",
        "  Y = conv2d(X)\n",
        "  # stripping the example and channel dimensions\n",
        "  return Y.reshape(Y.shape[2:])\n",
        "\n",
        "# 1 row and column are padded on either side, so total of 2 rows or columns are added\n",
        "conv2d = nn.LazyConv2d(1, kernel_size = 3, padding = 1)\n",
        "\n",
        "X = torch.rand(size = (8, 8))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thAIgP66WiQl",
        "outputId": "c1c138b9-e540-4afc-8238-92524ee34106"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a convolution layer with different height and width but selecting different padding to get the same output shape\n",
        "\n",
        "conv2d = nn.LazyConv2d(1, kernel_size = (5, 3), padding = (2, 1))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2MqDvt9YFoU",
        "outputId": "b2719359-97eb-47b4-ed93-c98144c0e941"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stride \n",
        "\n",
        "For computational efficiency, we might want to slide the conv layer more than 1 step at a time, skipping the intermediate locations. \n",
        "\n",
        "Number of rows and columns per traversed per slide is called stride. SO far we used strides 1. \n",
        "\n",
        "If the stride height is sh, stride width is sw, the output shape is \n",
        "\n",
        "$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$\n",
        "\n",
        "If we set ph = kh - 1 and pw = kw - 1, the output shape is simplified to \n",
        "\n",
        "$\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$. "
      ],
      "metadata": {
        "id": "ZkuCm_bRYghD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size = 3, padding = 1, stride = 2)\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqHsVfv6bGcJ",
        "outputId": "55d538d3-3693-4bfa-9fc7-41fa19aa2d1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size = (3, 5), padding = (0, 1), stride = (3, 4))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyKVNZvubMtQ",
        "outputId": "d4c58a83-ea9f-4698-bbc7-9822dcfde728"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Input and Multiple Output Channels\n",
        "\n",
        "Notion of channels is as old as CNNs themselves (Since 1995 almost)\n",
        "\n",
        "### Multiple Input Channels\n",
        "\n",
        "The convolution kernel must have the same number of channels as the input layer. The outputs of the kernel at each channel are then added that result in a single channel kernel output. \n"
      ],
      "metadata": {
        "id": "J4qlrD2UbV7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from d2l import torch as d2l\n",
        "\n",
        "def corr2d_multi_channel(X, K):\n",
        "  # Iterate through each dimension of K (each channel of K) then add them\n",
        "  return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
      ],
      "metadata": {
        "id": "xyWaRRhqefLW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
        "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
        "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
        "\n",
        "corr2d_multi_channel(X, K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neLDoa9XezFj",
        "outputId": "61f0deec-73b7-4791-c1d4-3757ce487b7e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  72.],\n",
              "        [104., 120.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Output Channels\n",
        "\n",
        "In the above example, no matter the number nof inputs, we end up getting a single dimension output. But it is essential to have multiple channel outputs. In most popular neural networks we add more channels to the output as we go in depth into the neural network architecture. This is done to compensate for the lack of spatial resolution. Intuitively, this can be thought of as each channel corresponding to a different set of features. \n",
        "\n",
        "If ci and co are the number of input and output channels, kh and kw are kernel height and width, to get multi channel kernel output we create kernel of shape ci x kh x kw for each output channel co. We concatenate them on the output channel dimension so that shape is co x ci x kh x kw. "
      ],
      "metadata": {
        "id": "K-p8Vwkve4TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def corr2d_multi_channel_kernel_output(X, K):\n",
        "  # Iterate through the 0th dimension of 'K' and each time perform \n",
        "  # cross correlation operations with input 'X'. All of the results are stacked\n",
        "  # together\n",
        "  return torch.stack([corr2d_multi_channel(X, k) for k in K], 0)\n"
      ],
      "metadata": {
        "id": "vJz5Qt-6f34b"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = torch.stack((K, K+1, K+2), 0)\n",
        "K.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MjaQ-NTgPOP",
        "outputId": "2ec02d3a-2c70-41ed-8ca1-85c5c9ea1625"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr2d_multi_channel_kernel_output(X, K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc6-cpeBgUXU",
        "outputId": "8d4c1bf2-42e1-423f-a039-4fddbd7bd019"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 56.,  72.],\n",
              "         [104., 120.]],\n",
              "\n",
              "        [[ 76., 100.],\n",
              "         [148., 172.]],\n",
              "\n",
              "        [[ 96., 128.],\n",
              "         [192., 224.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1x1 Convolutional Layer\n",
        "\n"
      ],
      "metadata": {
        "id": "T0OwyB51gaPs"
      }
    }
  ]
}